"""
Data Flow Analysis Engine for Dynamic Analysis Agent

This module analyzes how data moves through the application architecture,
identifying insecure data handling patterns and potential security vulnerabilities.
"""

import re
import requests
from typing import List, Dict, Any, Set, Optional, Tuple
from urllib.parse import urljoin, urlparse, parse_qs
from collections import defaultdict
# Using requests directly for HTTP operations

class DataFlowNode:
    """Represents a node in the data flow graph"""

    def __init__(self, node_type: str, location: str, data_type: str = 'unknown'):
        self.node_type = node_type  # 'input', 'processing', 'storage', 'output'
        self.location = location  # URL, function, file, etc.
        self.data_type = data_type  # 'string', 'integer', 'binary', 'json', etc.
        self.connections: List['DataFlowEdge'] = []
        self.properties: Dict[str, Any] = {}

class DataFlowEdge:
    """Represents a connection between data flow nodes"""

    def __init__(self, source: DataFlowNode, target: DataFlowNode,
                 transformation: str = 'direct', tainted: bool = False):
        self.source = source
        self.target = target
        self.transformation = transformation  # 'direct', 'encoded', 'encrypted', 'validated', etc.
        self.tainted = tainted
        self.properties: Dict[str, Any] = {}

class DataFlowGraph:
    """Graph representing data flow through the application"""

    def __init__(self):
        self.nodes: List[DataFlowNode] = []
        self.edges: List[DataFlowEdge] = []
        self.entry_points: List[DataFlowNode] = []
        self.exit_points: List[DataFlowNode] = []

    def add_node(self, node: DataFlowNode) -> DataFlowNode:
        """Add a node to the graph"""
        self.nodes.append(node)
        if node.node_type == 'input':
            self.entry_points.append(node)
        elif node.node_type == 'output':
            self.exit_points.append(node)
        return node

    def add_edge(self, edge: DataFlowEdge):
        """Add an edge to the graph"""
        self.edges.append(edge)
        edge.source.connections.append(edge)

    def find_paths(self, start_node: DataFlowNode, end_node: DataFlowNode,
                   max_depth: int = 10) -> List[List[DataFlowNode]]:
        """Find all paths from start node to end node"""
        paths = []
        visited = set()

        def dfs(current: DataFlowNode, path: List[DataFlowNode], depth: int):
            if depth > max_depth:
                return

            path.append(current)
            visited.add(current)

            if current == end_node:
                paths.append(path.copy())
            else:
                for edge in current.connections:
                    if edge.target not in visited:
                        dfs(edge.target, path, depth + 1)

            path.pop()
            visited.remove(current)

        dfs(start_node, [], 0)
        return paths

class DataFlowAnalyzer:
    """Data flow analysis engine"""

    def __init__(self, base_url: str):
        self.base_url = base_url
        self.graph = DataFlowGraph()
        self.vulnerabilities: List[Dict[str, Any]] = []
        self.analyzed_urls: Set[str] = set()

    def analyze_application(self) -> List[Dict[str, Any]]:
        """
        Perform comprehensive data flow analysis of the application

        Returns:
            List of discovered vulnerabilities
        """
        # Start with main page
        self._analyze_url(self.base_url)

        # Discover and analyze additional pages
        self._crawl_and_analyze()

        # Analyze the constructed data flow graph
        self._analyze_data_flows()

        return self.vulnerabilities

    def _analyze_url(self, url: str):
        """Analyze a single URL for data flow patterns"""
        if url in self.analyzed_urls:
            return

        self.analyzed_urls.add(url)

        try:
            response = requests.get(url, timeout=10)
            if response.status_code != 200:
                return

            content = response.text

            # Create input nodes for this URL
            input_node = DataFlowNode('input', url, 'http_request')
            self.graph.add_node(input_node)

            # Analyze different data flow patterns
            self._analyze_form_data_flow(content, url, input_node)
            self._analyze_javascript_data_flow(content, url, input_node)
            self._analyze_api_data_flow(content, url, input_node)
            self._analyze_storage_patterns(content, url, input_node)

        except Exception as e:
            pass

    def _analyze_form_data_flow(self, content: str, url: str, input_node: DataFlowNode):
        """Analyze data flow through HTML forms"""
        form_pattern = r'<form[^>]*>(.*?)</form>'
        input_pattern = r'<input[^>]*name=["\']([^"\']+)["\'][^>]*(?:value=["\']([^"\']*)["\'])?'
        select_pattern = r'<select[^>]*name=["\']([^"\']+)["\'][^>]*>(.*?)</select>'
        textarea_pattern = r'<textarea[^>]*name=["\']([^"\']+)["\'][^>]*>(.*?)</textarea>'

        forms = re.findall(form_pattern, content, re.DOTALL | re.IGNORECASE)

        for form_content in forms:
            # Find all form inputs
            inputs = re.findall(input_pattern, form_content, re.IGNORECASE)
            selects = re.findall(select_pattern, form_content, re.DOTALL | re.IGNORECASE)
            textareas = re.findall(textarea_pattern, form_content, re.DOTALL | re.IGNORECASE)

            all_inputs = inputs + selects + textareas

            for input_match in all_inputs:
                input_name = input_match[0]

                # Create processing node for form validation
                validation_node = DataFlowNode('processing', f'form_validation_{input_name}', 'validation')
                self.graph.add_node(validation_node)

                # Create edge from input to validation
                edge = DataFlowEdge(input_node, validation_node, 'form_input', tainted=True)
                self.graph.add_edge(edge)

                # Check for client-side validation
                if not self._has_client_validation(form_content, input_name):
                    self._report_vulnerability({
                        'type': 'insufficient_input_validation',
                        'severity': 'MEDIUM',
                        'description': f'Form input {input_name} lacks client-side validation',
                        'evidence': f'Form field: {input_name}',
                        'recommendation': 'Implement both client and server-side validation'
                    })

                # Check for dangerous input patterns
                self._check_dangerous_input_patterns(input_name, input_match)

    def _analyze_javascript_data_flow(self, content: str, url: str, input_node: DataFlowNode):
        """Analyze data flow through JavaScript code"""
        script_pattern = r'<script[^>]*>(.*?)</script>'
        scripts = re.findall(script_pattern, content, re.DOTALL | re.IGNORECASE)

        for script in scripts:
            if script.strip():
                # Create JavaScript processing node
                js_node = DataFlowNode('processing', f'javascript_{url}', 'javascript')
                self.graph.add_node(js_node)

                edge = DataFlowEdge(input_node, js_node, 'javascript_execution', tainted=True)
                self.graph.add_edge(edge)

                # Analyze JavaScript for dangerous patterns
                self._analyze_javascript_patterns(script, js_node)

    def _analyze_api_data_flow(self, content: str, url: str, input_node: DataFlowNode):
        """Analyze data flow through API calls"""
        # Look for AJAX calls, fetch requests, etc.
        api_patterns = [
            r'fetch\s*\(\s*["\']([^"\']+)["\']',
            r'\.ajax\s*\(\s*{\s*url\s*:\s*["\']([^"\']+)["\']',
            r'XMLHttpRequest\s*\(\s*\)\s*;?\s*[^}]*open\s*\(\s*["\']([^"\']+)["\']',
            r'\$\.(?:get|post|ajax)\s*\(\s*["\']([^"\']+)["\']',
        ]

        for pattern in api_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for api_url in matches:
                # Create API node
                api_node = DataFlowNode('processing', f'api_call_{api_url}', 'api_request')
                self.graph.add_node(api_node)

                edge = DataFlowEdge(input_node, api_node, 'api_request', tainted=True)
                self.graph.add_edge(edge)

                # Check if API URL is parameterized
                if '?' in api_url or '{' in api_url:
                    self._check_api_parameter_security(api_url)

    def _analyze_storage_patterns(self, content: str, url: str, input_node: DataFlowNode):
        """Analyze data storage patterns (localStorage, cookies, etc.)"""
        storage_patterns = [
            (r'localStorage\.setItem\s*\(\s*["\']([^"\']+)["\']\s*,\s*([^)]+)', 'localStorage'),
            (r'sessionStorage\.setItem\s*\(\s*["\']([^"\']+)["\']\s*,\s*([^)]+)', 'sessionStorage'),
            (r'document\.cookie\s*=\s*([^;]+)', 'cookie'),
        ]

        for pattern, storage_type in storage_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                key = match[0] if len(match) > 1 else 'unknown'
                value_expr = match[1] if len(match) > 1 else match[0]

                # Create storage node
                storage_node = DataFlowNode('storage', f'{storage_type}_{key}', storage_type)
                self.graph.add_node(storage_node)

                edge = DataFlowEdge(input_node, storage_node, 'data_storage', tainted=True)
                self.graph.add_edge(edge)

                # Check for sensitive data storage
                if any(sensitive in key.lower() for sensitive in ['password', 'token', 'key', 'secret']):
                    self._report_vulnerability({
                        'type': 'sensitive_data_storage',
                        'severity': 'HIGH',
                        'description': f'Sensitive data stored in {storage_type}',
                        'evidence': f'Storage key: {key}',
                        'recommendation': 'Avoid storing sensitive data in client-side storage'
                    })

    def _has_client_validation(self, form_content: str, input_name: str) -> bool:
        """Check if a form input has client-side validation"""
        # Look for validation attributes
        validation_attrs = [
            'required',
            'pattern',
            'minlength',
            'maxlength',
            'min',
            'max',
            'type="email"',
            'type="url"',
        ]

        input_pattern = f'<input[^>]*name=["\']{re.escape(input_name)}["\'][^>]*'
        input_match = re.search(input_pattern, form_content, re.IGNORECASE)

        if input_match:
            input_tag = input_match.group(0)
            for attr in validation_attrs:
                if attr in input_tag:
                    return True

        return False

    def _check_dangerous_input_patterns(self, input_name: str, input_match: Tuple):
        """Check for potentially dangerous input patterns"""
        dangerous_names = [
            ('id', 'Potential IDOR vulnerability'),
            ('file', 'File upload without validation'),
            ('path', 'Path traversal vulnerability'),
            ('sql', 'SQL injection risk'),
            ('cmd', 'Command injection risk'),
            ('exec', 'Code execution risk'),
        ]

        for dangerous_name, description in dangerous_names:
            if dangerous_name in input_name.lower():
                self._report_vulnerability({
                    'type': 'dangerous_input_field',
                    'severity': 'MEDIUM',
                    'description': f'{description} in field {input_name}',
                    'evidence': f'Input field: {input_name}',
                    'recommendation': 'Implement strict validation for this input field'
                })

    def _analyze_javascript_patterns(self, script: str, js_node: DataFlowNode):
        """Analyze JavaScript code for data flow issues"""
        dangerous_patterns = [
            (r'eval\s*\(', 'code_execution', 'CRITICAL', 'Code execution via eval()'),
            (r'Function\s*\(', 'code_execution', 'CRITICAL', 'Dynamic function creation'),
            (r'innerHTML\s*=', 'html_injection', 'HIGH', 'HTML injection via innerHTML'),
            (r'document\.write\s*\(', 'html_injection', 'HIGH', 'HTML injection via document.write'),
            (r'location\.href\s*=', 'open_redirect', 'MEDIUM', 'Client-side redirect'),
            (r'window\.open\s*\(', 'popup_injection', 'LOW', 'Potential popup manipulation'),
        ]

        for pattern, vuln_type, severity, description in dangerous_patterns:
            if re.search(pattern, script, re.IGNORECASE):
                self._report_vulnerability({
                    'type': vuln_type,
                    'severity': severity,
                    'description': f'{description} detected in JavaScript',
                    'evidence': f'Pattern: {pattern}',
                    'recommendation': 'Avoid using dangerous JavaScript patterns'
                })

    def _check_api_parameter_security(self, api_url: str):
        """Check if API parameters are properly secured"""
        # Check for common API security issues
        if re.search(r'\{[^}]+\}', api_url):  # Template parameters
            self._report_vulnerability({
                'type': 'api_parameter_injection',
                'severity': 'MEDIUM',
                'description': 'API endpoint uses template parameters',
                'evidence': f'API URL: {api_url}',
                'recommendation': 'Validate and sanitize API parameters'
            })

        if '?' in api_url:  # Query parameters
            params = parse_qs(urlparse(api_url).query)
            for param_name in params.keys():
                if any(sensitive in param_name.lower() for sensitive in ['id', 'user', 'admin']):
                    self._report_vulnerability({
                        'type': 'sensitive_data_in_url',
                        'severity': 'LOW',
                        'description': f'Sensitive parameter {param_name} exposed in URL',
                        'evidence': f'Parameter: {param_name}',
                        'recommendation': 'Avoid exposing sensitive data in URLs'
                    })

    def _analyze_data_flows(self):
        """Analyze the constructed data flow graph for vulnerabilities"""
        # Look for paths from untrusted inputs to sensitive operations
        for entry_point in self.graph.entry_points:
            for exit_point in self.graph.exit_points:
                paths = self.graph.find_paths(entry_point, exit_point)

                for path in paths:
                    # Check if path goes through dangerous transformations
                    dangerous_path = self._analyze_path_security(path)
                    if dangerous_path:
                        self._report_vulnerability(dangerous_path)

    def _analyze_path_security(self, path: List[DataFlowNode]) -> Optional[Dict[str, Any]]:
        """Analyze a data flow path for security issues"""
        has_validation = False
        has_sanitization = False
        reaches_dangerous_sink = False

        for node in path:
            if node.node_type == 'processing':
                if 'validation' in node.location.lower():
                    has_validation = True
                if 'sanitize' in node.location.lower() or 'escape' in node.location.lower():
                    has_sanitization = True

            elif node.node_type == 'output':
                if any(dangerous in node.location.lower() for dangerous in
                       ['sql', 'command', 'file', 'eval', 'exec']):
                    reaches_dangerous_sink = True

        if reaches_dangerous_sink and not (has_validation or has_sanitization):
            return {
                'type': 'insecure_data_flow',
                'severity': 'HIGH',
                'description': 'Data flows from untrusted input to dangerous operation without validation',
                'evidence': f'Path: {" -> ".join([n.location for n in path])}',
                'recommendation': 'Implement input validation and data sanitization'
            }

        return None

    def _crawl_and_analyze(self, max_pages: int = 10):
        """Crawl the application and analyze additional pages"""
        visited = set([self.base_url])
        to_visit = [self.base_url]

        while to_visit and len(visited) < max_pages:
            current_url = to_visit.pop(0)

            try:
                response = requests.get(current_url, timeout=10)
                if response.status_code != 200:
                    continue

                # Extract links to other pages
                link_pattern = r'href=["\']([^"\']+)["\']'
                links = re.findall(link_pattern, response.text, re.IGNORECASE)

                for link in links[:5]:  # Limit to avoid excessive crawling
                    if not link.startswith(('http://', 'https://', 'mailto:', 'javascript:', '#')):
                        full_url = urljoin(current_url, link)
                        if (urlparse(full_url).netloc == urlparse(self.base_url).netloc and
                            full_url not in visited):
                            visited.add(full_url)
                            to_visit.append(full_url)
                            self._analyze_url(full_url)

            except Exception:
                continue

    def _report_vulnerability(self, vuln: Dict[str, Any]):
        """Report a discovered vulnerability"""
        # Avoid duplicates
        for existing_vuln in self.vulnerabilities:
            if (existing_vuln.get('type') == vuln.get('type') and
                existing_vuln.get('evidence') == vuln.get('evidence')):
                return

        self.vulnerabilities.append(vuln)


def test_data_flow_analysis(base_url: str) -> List[Dict[str, Any]]:
    """
    Test for vulnerabilities using data flow analysis

    Args:
        base_url: Base URL to test

    Returns:
        List of discovered vulnerabilities
    """
    analyzer = DataFlowAnalyzer(base_url)
    return analyzer.analyze_application()
